{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "#import demoji\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis program assumes that your input file contains only tweet content and nothing else\\npart 1 - removes hashtags, mentions and urls\\npart 2 - tokenisation to get vocabulary and counts\\npart 3 - removes low frequency words from vocabulary\\npart 4 - replaces OOV words by UNK token\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This program assumes that your input file contains only tweet content and nothing else\n",
    "part 1 - removes hashtags, mentions and urls\n",
    "part 2 - tokenisation to get vocabulary and counts\n",
    "part 3 - removes low frequency words from vocabulary\n",
    "part 4 - replaces OOV words by UNK token\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1\n",
    "ff=open('data/archive/2019_03/tweets_clean.tsv','r').readlines()\n",
    "gg = open('data/archive/2019_03/tweets_cleaned.tsv','w')\n",
    "for txt in ff:\n",
    "    if txt == '\\n':\n",
    "        continue\n",
    "    #print (txt)\n",
    "    txt = txt.lower()\n",
    "    #txt = re.sub(r'\\#[a-zA-Z0-9]+', \" <HASH> \", txt)\n",
    "    txt = re.sub(r'\\#', \" <HASH> \", txt)\n",
    "    txt = re.sub(r'\\@.[a-zA-Z0-9]\\S+', \" <MENTION> \", txt)\n",
    "    txt = re.sub(r'https?:\\/\\/.\\S*', \" <URL> \", txt)\n",
    "    #txt = demoji.replace(txt,\" <EMOJI> \")\n",
    "    txt = re.sub(r'\\s+',\" \",txt)\n",
    "    \n",
    "    #tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", txt.lower())).strip()\n",
    "    #print (tweet)\n",
    "    #txt = re.sub(r'[[\\U0001F600-\\U0001F64F]|[\\U0001F300-\\U0001F5FF]|[\\U0001F680-\\U0001F6FF]|[\\U0001F1E0-\\U0001F1FF]|[\\U00002702-\\U000027B0]|[\\U000024C2-\\U0001F251]]+',\"<EMOJI>\", txt)\n",
    "    gg.write(txt+'\\n')\n",
    "gg.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2\n",
    "from nltk import ngrams, FreqDist\n",
    "#all_counts = dict()\n",
    "ff=open('data/archive/2019_03/tweets_cleaned.tsv','r').readlines()\n",
    "#all_counts = FreqDist(ngrams(ff[:9000], 3))\n",
    "#print (sorted(all_counts))\n",
    "vocab = dict()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "for tweet in ff:\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token in vocab.keys():\n",
    "            vocab[token] += 1\n",
    "        else:\n",
    "            vocab[token] = 1\n",
    "#sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "#sorted_d = dict(sorted(vocab.items(), key=operator.itemgetter(1)))\n",
    "#print (type(vocab))\n",
    "#print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52182\n"
     ]
    }
   ],
   "source": [
    "#part 3\n",
    "oov=[]\n",
    "vocab_fin=['<UNK>']\n",
    "for key in vocab.keys():\n",
    "    #print (pair)\n",
    "    if vocab[key]>9:\n",
    "        vocab_fin.append(key)\n",
    "        \n",
    "print(len(vocab_fin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 4\n",
    "ff=open('data/archive/2019_03/tweets_cleaned.tsv','r').readlines()\n",
    "gg=open('data/archive/2019_03/tweets_processed.tsv','w')\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "for tweet in ff:\n",
    "    new = ''\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token not in vocab_fin:\n",
    "            new += '<UNK> '\n",
    "        else:\n",
    "            new += token+' '\n",
    "    #print (new)\n",
    "    gg.write(new+'\\n')\n",
    "gg.close()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unnecessary "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
