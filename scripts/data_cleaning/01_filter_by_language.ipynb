{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Filter Tweets by Language\n",
    "\n",
    "This notebook has code to filter the archive.org twitter dumps by language and stores in two tsv files. One of them has all of the Tweets and the other one excludes Retweets. To use it, extract the tar file for a given day and a folder with a number `XX` will appear.\n",
    "\n",
    "This file must be in the same directory as that one. Within that folder there will be one for each hour of the day and within those there will be a json.bz2 file for each minute of the hour. Those must be extracted so the json files for each minute are in the corresponding hour directory.\n",
    "\n",
    "This piece of code imports the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change these according to the name of the day and to the laguage that you want to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_dir  = \"02\"\n",
    "language = \"es\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code imports the Tweets and filters them by language.\n",
    "\n",
    "If `Not Found` is displayed, the json file was most likely not extracted correctly. Some of the json files of the beginning of the 00 hour might be missing, which is expected. Any other of these mistakes is not.\n",
    "\n",
    "The messages `Backslash Character Found` and `Tab Character Found` appear to tell you that you should be careful with the output so that no rogue character breaks the final tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Importando datos de la hora 00\n",
      "Not Found: 00 00\n",
      "Not Found: 00 01\n",
      "Not Found: 00 02\n",
      "Not Found: 00 03\n",
      "Not Found: 00 04\n",
      "Not Found: 00 05\n",
      "Not Found: 00 06\n",
      "Not Found: 00 07\n",
      "Not Found: 00 08\n",
      "Not Found: 00 10\n",
      "Not Found: 00 11\n",
      "Not Found: 00 12\n",
      "Not Found: 00 13\n",
      "Not Found: 00 14\n",
      "Not Found: 00 15\n",
      "Not Found: 00 16\n",
      "Not Found: 00 17\n",
      "Not Found: 00 18\n",
      "Not Found: 00 20\n",
      "Not Found: 00 21\n",
      "Not Found: 00 22\n",
      "Not Found: 00 23\n",
      "Not Found: 00 24\n",
      "Not Found: 00 25\n",
      "Not Found: 00 26\n",
      "Not Found: 00 27\n",
      "Not Found: 00 28\n",
      "\n",
      "Importando datos de la hora 01\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 02\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 03\n",
      "\n",
      "Importando datos de la hora 04\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 05\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 06\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 07\n",
      "\n",
      "Importando datos de la hora 08\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 09\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 10\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 11\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 12\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 13\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 14\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 15\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 16\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 17\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 18\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 19\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 20\n",
      "\n",
      "Importando datos de la hora 21\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 22\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Importando datos de la hora 23\n",
      "Backslash found\n",
      "Backslash found\n",
      "\n",
      "Total de Twits filtrados:\n",
      "216784\n"
     ]
    }
   ],
   "source": [
    "path = \"./\" + day_dir + \"/\"\n",
    "\n",
    "tweets = []\n",
    "\n",
    "for a in range(0,3):\n",
    "    for b in range(0,10):\n",
    "        if a*10 + b > 23:\n",
    "            continue\n",
    "        hour = str(a) + str(b)\n",
    "        print(\"\\nImporting data from hour\", hour)\n",
    "        for d in range(0,5):\n",
    "            for u in range(0,9):\n",
    "                minute = str(d) + str(u)\n",
    "                file = path + hour + \"/\" + minute + \".json\"\n",
    "                try:\n",
    "                    for line in open(file, 'r'):\n",
    "                        tweet = json.loads(line)\n",
    "                        if (\"lang\" in tweet.keys()) and (tweet[\"lang\"]==language):\n",
    "                            tweets.append(tweet)\n",
    "                            if (\"\\t\" in tweet[\"text\"]):\n",
    "                                print(\"Tab character found!\")\n",
    "                            if (\"\\\\\" in tweet[\"text\"]):\n",
    "                                print(\"Backslash found\")\n",
    "                except FileNotFoundError:\n",
    "                    print(\"Not Found:\" , hour, minute)\n",
    "\n",
    "print(\"\\nTotal Twits found:\")\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part checks whether all metadata is accounted for. The `keep` list is the one I would consider useful and the ones in `special` are the ones that could be useful to either further prune the data or to actually keept the proper fields. What you do with these is up to you. An output other than `[]` means that at least one of your entries has metadata that hadn't appeared on any of my runs of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TR']\n",
      "['DE']\n",
      "['withheld_in_countries']\n"
     ]
    }
   ],
   "source": [
    "nokeep = [\"user\", \"geo\", \"coordinates\", \"quote_count\", \"contributors\",\"reply_count\",\"retweet_count\", \"favorited\",\n",
    "          \"retweeted\", \"in_reply_to_status_id\", \"in_reply_to_status_id_str\", \"id_str\", \"created_at\", \"favorite_count\",\n",
    "          \"in_reply_to_user_id\", \"in_reply_to_user_id_str\", \"in_reply_to_screen_name\", \"display_text_range\", \"source\",\n",
    "          \"timestamp_ms\", \"retweeted_status\", \"entities\", \"extended_entities\", \"delete\", \"truncated\", \"is_quote_status\",\n",
    "          \"extended_tweet\", \"filter_level\", \"possibly_sensitive\", \"quoted_status_id\", \"quoted_status_id_str\",\n",
    "          \"quoted_status\", \"quoted_status_permalink\", \"TR\", \"DE\", \"withheld_in_countries\"]\n",
    "\n",
    "special = [\"truncated\", \"is_quote_status\", \"extended_tweet\"]\n",
    "\n",
    "keep = [\"id\", \"text\", \"lang\", \"place\"]\n",
    "\n",
    "other = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    for key in tweet.keys():\n",
    "        if key not in nokeep+keep:\n",
    "            check = \"withheld_in_countries\"\n",
    "            if key==check and tweet[check]!=False:\n",
    "                print(tweet[check])\n",
    "            if key not in other:\n",
    "                other.append(key)\n",
    "            \n",
    "print(other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we save the Tweets in a `XX.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets)\n",
    "df = df.set_index(\"id\")    \n",
    "df = df.fillna(\"\")\n",
    "df.to_csv(day_dir+\".tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This other part filters Retweets and then saves them into a `XX_clean.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.loc[pd.notnull(df[\"retweeted_status\"])]\n",
    "df_clean = df_clean.loc[df[\"retweeted_status\"]==\"\"]\n",
    "df_clean.to_csv(day_dir+\"_clean.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both numbers are different, that means that there were no Retweets in that minute. Depending on the language that you are dealing with, that might be highly unlikely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216784\n",
      "87869\n"
     ]
    }
   ],
   "source": [
    "print(df.shape[0])\n",
    "print(df_clean.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
