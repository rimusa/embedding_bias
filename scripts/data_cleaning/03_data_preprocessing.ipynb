{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "This script is based on the preprocessing script that Mughda made (https://github.com/seraphinatarrant/embedding_bias/tree/Mugdha). I moved all of the code to ``data_cleaning`` so that it could be more easily accessible in case one of the other scripts needs it. Other than that, I only changed the code so that it would also be usable with csv files.\n",
    "\n",
    "As for the structure of the data, in most of the other scripts I will assume that you saved the data to ``data/archives/2019_03``. If you want to change this, you will have to change that from all of the other scripts. Note that the rest of the code should not have any of that hardcoded for ease of code recycling.\n",
    "\n",
    "We first import all of the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import Preprocessing as pre\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is only for debugging and should be removed before uploading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = reload(pre)\n",
    "preprocess_file = pre.preprocess_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean and preprocess the embeddings dataset. This also generates the vocabulary file that will be used for cleaning the downstream dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "\n",
      "Cleaning data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-12526afe6a37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"processed.tsv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\embedding_bias\\scripts\\data_cleaning\\Preprocessing.py\u001b[0m in \u001b[0;36mpreprocess_file\u001b[1;34m(origin_path, save_path, file_type, column, vocab)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nCleaning data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nVocabulary generated\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\embedding_bias\\scripts\\data_cleaning\\Preprocessing.py\u001b[0m in \u001b[0;36mcleaner\u001b[1;34m(data, threshold, gen_vocab)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# Cleans the Tweet and turns it back into a string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[0mcleaned_tweets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\embedding_bias\\scripts\\data_cleaning\\Preprocessing.py\u001b[0m in \u001b[0;36mclean_tweet\u001b[1;34m(text, vocab, tokenizer)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m# Tokenize the cleaned string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;31m# Add the token counts into the vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gensim\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreduce_lengthening\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;31m# Shorten problematic sequences of characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0msafe_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHANG_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\1\\1\\1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;31m# Tokenize:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWORD_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"../../data/archive/2019_03/tweets_\"\n",
    "\n",
    "origin_path = path + \"clean.tsv\"\n",
    "save_path = path + \"processed.tsv\"\n",
    "\n",
    "vocab = preprocess_file(origin_path, save_path, \"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary length sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate the preprocessed dataset for the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "\n",
      "Cleaning data...\n",
      "1000 of 1000 Tweets cleaned\n",
      "\n",
      "Vocabulary generated\n",
      "\n",
      "5149 words in the vocabulary\n",
      "\n",
      "Preprocessing data...\n",
      "1000 of 1000 Tweets processed\n",
      "1000 of 1000 Tweets processed\n",
      "\n",
      "Saving data...\n",
      "Data successfully saved!\n",
      "\n",
      "\n",
      "\n",
      "Importing data...\n",
      "\n",
      "Cleaning data...\n",
      "9000 of 9000 Tweets cleaned\n",
      "\n",
      "Vocabulary generated\n",
      "\n",
      "18075 words in the vocabulary\n",
      "\n",
      "Preprocessing data...\n",
      "1000 of 9000 Tweets processed\n",
      "2000 of 9000 Tweets processed\n",
      "3000 of 9000 Tweets processed\n",
      "4000 of 9000 Tweets processed\n",
      "5000 of 9000 Tweets processed\n",
      "6000 of 9000 Tweets processed\n",
      "7000 of 9000 Tweets processed\n",
      "8000 of 9000 Tweets processed\n",
      "9000 of 9000 Tweets processed\n",
      "9000 of 9000 Tweets processed\n",
      "\n",
      "Saving data...\n",
      "Data successfully saved!\n",
      "\n",
      "\n",
      "\n",
      "Importing data...\n",
      "\n",
      "Cleaning data...\n",
      "3000 of 3000 Tweets cleaned\n",
      "\n",
      "Vocabulary generated\n",
      "\n",
      "8862 words in the vocabulary\n",
      "\n",
      "Preprocessing data...\n",
      "1000 of 3000 Tweets processed\n",
      "2000 of 3000 Tweets processed\n",
      "3000 of 3000 Tweets processed\n",
      "3000 of 3000 Tweets processed\n",
      "\n",
      "Saving data...\n",
      "Data successfully saved!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "origin = '../../data/hateval2019/hateval2019_en_'\n",
    "save = '../../data/hateval2019/hateval2019_clean_en_'\n",
    "files = [\"dev\", \"train\", \"test\"]\n",
    "\n",
    "for file in files:\n",
    "    origin_path = origin + file + \".csv\"\n",
    "    save_path = save + file + \".csv\"\n",
    "\n",
    "    preprocess_file(origin_path, save_path, \"DataFrame\")#, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(save_path, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                               text  HS  TR  AG\n",
      "0  34243  <MENTION> <MENTION> <MENTION> oh , i could hav...   0   0   0\n",
      "1  30593  several of the wild fires in <HASH> california...   0   0   0\n",
      "2  31427  <MENTION> my question is how do you resettle a...   0   0   0\n",
      "3  31694  <HASH> europe , you've got a problem ! we must...   1   0   0\n",
      "4  31865  this is outrageous ! <HASH> stopillegalimmigra...   1   0   0\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
