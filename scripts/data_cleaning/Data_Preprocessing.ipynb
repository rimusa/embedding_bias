{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "This script is based on the preprocessing script that Mughda made (https://github.com/seraphinatarrant/embedding_bias/tree/Mugdha). I moved all of the code to ``./data_preprocessing`` so that it could be more easily accessible in case one of the other scripts needs it. Other than that, I only changed the code so that it would also be usable with csv files.\n",
    "\n",
    "We first import all of the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import Preprocessing as pre\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is only for debugging and should be removed before uploading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = reload(pre)\n",
    "preprocess_txt_file = pre.preprocess_txt_file\n",
    "preprocess_DataFrame = pre.preprocess_DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean and preprocess the embeddings dataset. This also generates the vocabulary file that will be used for cleaning the downstream dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Cleaning data...\n",
      "10000 of 1497220 Tweets cleaned\n",
      "20000 of 1497220 Tweets cleaned\n",
      "30000 of 1497220 Tweets cleaned\n",
      "40000 of 1497220 Tweets cleaned\n",
      "50000 of 1497220 Tweets cleaned\n",
      "60000 of 1497220 Tweets cleaned\n",
      "70000 of 1497220 Tweets cleaned\n",
      "80000 of 1497220 Tweets cleaned\n",
      "90000 of 1497220 Tweets cleaned\n",
      "100000 of 1497220 Tweets cleaned\n",
      "110000 of 1497220 Tweets cleaned\n",
      "120000 of 1497220 Tweets cleaned\n",
      "130000 of 1497220 Tweets cleaned\n",
      "140000 of 1497220 Tweets cleaned\n",
      "150000 of 1497220 Tweets cleaned\n",
      "160000 of 1497220 Tweets cleaned\n",
      "170000 of 1497220 Tweets cleaned\n",
      "180000 of 1497220 Tweets cleaned\n",
      "190000 of 1497220 Tweets cleaned\n",
      "200000 of 1497220 Tweets cleaned\n",
      "210000 of 1497220 Tweets cleaned\n",
      "220000 of 1497220 Tweets cleaned\n",
      "230000 of 1497220 Tweets cleaned\n",
      "240000 of 1497220 Tweets cleaned\n",
      "250000 of 1497220 Tweets cleaned\n",
      "260000 of 1497220 Tweets cleaned\n",
      "270000 of 1497220 Tweets cleaned\n",
      "280000 of 1497220 Tweets cleaned\n",
      "290000 of 1497220 Tweets cleaned\n",
      "300000 of 1497220 Tweets cleaned\n",
      "310000 of 1497220 Tweets cleaned\n",
      "320000 of 1497220 Tweets cleaned\n",
      "330000 of 1497220 Tweets cleaned\n",
      "340000 of 1497220 Tweets cleaned\n",
      "350000 of 1497220 Tweets cleaned\n",
      "360000 of 1497220 Tweets cleaned\n",
      "370000 of 1497220 Tweets cleaned\n",
      "380000 of 1497220 Tweets cleaned\n",
      "390000 of 1497220 Tweets cleaned\n",
      "400000 of 1497220 Tweets cleaned\n",
      "410000 of 1497220 Tweets cleaned\n",
      "420000 of 1497220 Tweets cleaned\n",
      "430000 of 1497220 Tweets cleaned\n",
      "440000 of 1497220 Tweets cleaned\n",
      "450000 of 1497220 Tweets cleaned\n",
      "460000 of 1497220 Tweets cleaned\n",
      "470000 of 1497220 Tweets cleaned\n",
      "480000 of 1497220 Tweets cleaned\n",
      "490000 of 1497220 Tweets cleaned\n",
      "500000 of 1497220 Tweets cleaned\n",
      "510000 of 1497220 Tweets cleaned\n",
      "520000 of 1497220 Tweets cleaned\n",
      "530000 of 1497220 Tweets cleaned\n",
      "540000 of 1497220 Tweets cleaned\n",
      "550000 of 1497220 Tweets cleaned\n",
      "560000 of 1497220 Tweets cleaned\n",
      "570000 of 1497220 Tweets cleaned\n",
      "580000 of 1497220 Tweets cleaned\n",
      "590000 of 1497220 Tweets cleaned\n",
      "600000 of 1497220 Tweets cleaned\n",
      "610000 of 1497220 Tweets cleaned\n",
      "620000 of 1497220 Tweets cleaned\n",
      "630000 of 1497220 Tweets cleaned\n",
      "640000 of 1497220 Tweets cleaned\n",
      "650000 of 1497220 Tweets cleaned\n",
      "660000 of 1497220 Tweets cleaned\n",
      "670000 of 1497220 Tweets cleaned\n",
      "680000 of 1497220 Tweets cleaned\n",
      "690000 of 1497220 Tweets cleaned\n",
      "700000 of 1497220 Tweets cleaned\n",
      "710000 of 1497220 Tweets cleaned\n",
      "720000 of 1497220 Tweets cleaned\n",
      "730000 of 1497220 Tweets cleaned\n",
      "740000 of 1497220 Tweets cleaned\n",
      "750000 of 1497220 Tweets cleaned\n",
      "760000 of 1497220 Tweets cleaned\n",
      "770000 of 1497220 Tweets cleaned\n",
      "780000 of 1497220 Tweets cleaned\n",
      "790000 of 1497220 Tweets cleaned\n",
      "800000 of 1497220 Tweets cleaned\n",
      "810000 of 1497220 Tweets cleaned\n",
      "820000 of 1497220 Tweets cleaned\n",
      "830000 of 1497220 Tweets cleaned\n",
      "840000 of 1497220 Tweets cleaned\n",
      "850000 of 1497220 Tweets cleaned\n",
      "860000 of 1497220 Tweets cleaned\n",
      "870000 of 1497220 Tweets cleaned\n",
      "880000 of 1497220 Tweets cleaned\n",
      "890000 of 1497220 Tweets cleaned\n",
      "900000 of 1497220 Tweets cleaned\n",
      "910000 of 1497220 Tweets cleaned\n",
      "920000 of 1497220 Tweets cleaned\n",
      "930000 of 1497220 Tweets cleaned\n",
      "940000 of 1497220 Tweets cleaned\n",
      "950000 of 1497220 Tweets cleaned\n",
      "960000 of 1497220 Tweets cleaned\n",
      "970000 of 1497220 Tweets cleaned\n",
      "980000 of 1497220 Tweets cleaned\n",
      "990000 of 1497220 Tweets cleaned\n",
      "1000000 of 1497220 Tweets cleaned\n",
      "1010000 of 1497220 Tweets cleaned\n",
      "1020000 of 1497220 Tweets cleaned\n",
      "1030000 of 1497220 Tweets cleaned\n",
      "1040000 of 1497220 Tweets cleaned\n",
      "1050000 of 1497220 Tweets cleaned\n",
      "1060000 of 1497220 Tweets cleaned\n",
      "1070000 of 1497220 Tweets cleaned\n",
      "1080000 of 1497220 Tweets cleaned\n",
      "1090000 of 1497220 Tweets cleaned\n",
      "1100000 of 1497220 Tweets cleaned\n",
      "1110000 of 1497220 Tweets cleaned\n",
      "1120000 of 1497220 Tweets cleaned\n",
      "1130000 of 1497220 Tweets cleaned\n",
      "1140000 of 1497220 Tweets cleaned\n",
      "1150000 of 1497220 Tweets cleaned\n",
      "1160000 of 1497220 Tweets cleaned\n",
      "1170000 of 1497220 Tweets cleaned\n",
      "1180000 of 1497220 Tweets cleaned\n",
      "1190000 of 1497220 Tweets cleaned\n",
      "1200000 of 1497220 Tweets cleaned\n",
      "1210000 of 1497220 Tweets cleaned\n",
      "1220000 of 1497220 Tweets cleaned\n",
      "1230000 of 1497220 Tweets cleaned\n",
      "1240000 of 1497220 Tweets cleaned\n",
      "1250000 of 1497220 Tweets cleaned\n",
      "1260000 of 1497220 Tweets cleaned\n",
      "1270000 of 1497220 Tweets cleaned\n",
      "1280000 of 1497220 Tweets cleaned\n",
      "1290000 of 1497220 Tweets cleaned\n",
      "1300000 of 1497220 Tweets cleaned\n",
      "1310000 of 1497220 Tweets cleaned\n",
      "1320000 of 1497220 Tweets cleaned\n",
      "1330000 of 1497220 Tweets cleaned\n",
      "1340000 of 1497220 Tweets cleaned\n",
      "1350000 of 1497220 Tweets cleaned\n",
      "1360000 of 1497220 Tweets cleaned\n",
      "1370000 of 1497220 Tweets cleaned\n",
      "1380000 of 1497220 Tweets cleaned\n",
      "1390000 of 1497220 Tweets cleaned\n",
      "1400000 of 1497220 Tweets cleaned\n",
      "1410000 of 1497220 Tweets cleaned\n",
      "1420000 of 1497220 Tweets cleaned\n",
      "1430000 of 1497220 Tweets cleaned\n",
      "1440000 of 1497220 Tweets cleaned\n",
      "1450000 of 1497220 Tweets cleaned\n",
      "1460000 of 1497220 Tweets cleaned\n",
      "1470000 of 1497220 Tweets cleaned\n",
      "1480000 of 1497220 Tweets cleaned\n",
      "1490000 of 1497220 Tweets cleaned\n",
      "Vocabulary generated\n",
      "52499 words in the vocabulary\n",
      "Preprocessing data...\n",
      "1000 of 1497220 Tweets processed\n",
      "2000 of 1497220 Tweets processed\n",
      "3000 of 1497220 Tweets processed\n",
      "4000 of 1497220 Tweets processed\n",
      "5000 of 1497220 Tweets processed\n",
      "6000 of 1497220 Tweets processed\n",
      "7000 of 1497220 Tweets processed\n",
      "8000 of 1497220 Tweets processed\n",
      "9000 of 1497220 Tweets processed\n",
      "10000 of 1497220 Tweets processed\n",
      "11000 of 1497220 Tweets processed\n",
      "12000 of 1497220 Tweets processed\n",
      "13000 of 1497220 Tweets processed\n",
      "14000 of 1497220 Tweets processed\n",
      "15000 of 1497220 Tweets processed\n",
      "16000 of 1497220 Tweets processed\n",
      "17000 of 1497220 Tweets processed\n",
      "18000 of 1497220 Tweets processed\n",
      "19000 of 1497220 Tweets processed\n",
      "20000 of 1497220 Tweets processed\n",
      "21000 of 1497220 Tweets processed\n",
      "22000 of 1497220 Tweets processed\n",
      "23000 of 1497220 Tweets processed\n",
      "24000 of 1497220 Tweets processed\n",
      "25000 of 1497220 Tweets processed\n",
      "26000 of 1497220 Tweets processed\n",
      "27000 of 1497220 Tweets processed\n",
      "28000 of 1497220 Tweets processed\n",
      "29000 of 1497220 Tweets processed\n",
      "30000 of 1497220 Tweets processed\n",
      "31000 of 1497220 Tweets processed\n",
      "32000 of 1497220 Tweets processed\n",
      "33000 of 1497220 Tweets processed\n",
      "34000 of 1497220 Tweets processed\n",
      "35000 of 1497220 Tweets processed\n",
      "36000 of 1497220 Tweets processed\n",
      "37000 of 1497220 Tweets processed\n",
      "38000 of 1497220 Tweets processed\n",
      "39000 of 1497220 Tweets processed\n",
      "40000 of 1497220 Tweets processed\n",
      "41000 of 1497220 Tweets processed\n",
      "42000 of 1497220 Tweets processed\n",
      "43000 of 1497220 Tweets processed\n",
      "44000 of 1497220 Tweets processed\n",
      "45000 of 1497220 Tweets processed\n",
      "46000 of 1497220 Tweets processed\n",
      "47000 of 1497220 Tweets processed\n",
      "48000 of 1497220 Tweets processed\n",
      "49000 of 1497220 Tweets processed\n",
      "50000 of 1497220 Tweets processed\n",
      "51000 of 1497220 Tweets processed\n",
      "52000 of 1497220 Tweets processed\n",
      "53000 of 1497220 Tweets processed\n",
      "54000 of 1497220 Tweets processed\n",
      "55000 of 1497220 Tweets processed\n",
      "56000 of 1497220 Tweets processed\n",
      "57000 of 1497220 Tweets processed\n",
      "58000 of 1497220 Tweets processed\n",
      "59000 of 1497220 Tweets processed\n",
      "60000 of 1497220 Tweets processed\n",
      "61000 of 1497220 Tweets processed\n",
      "62000 of 1497220 Tweets processed\n",
      "63000 of 1497220 Tweets processed\n",
      "64000 of 1497220 Tweets processed\n",
      "65000 of 1497220 Tweets processed\n",
      "66000 of 1497220 Tweets processed\n",
      "67000 of 1497220 Tweets processed\n",
      "68000 of 1497220 Tweets processed\n",
      "69000 of 1497220 Tweets processed\n",
      "70000 of 1497220 Tweets processed\n",
      "71000 of 1497220 Tweets processed\n",
      "72000 of 1497220 Tweets processed\n",
      "73000 of 1497220 Tweets processed\n",
      "74000 of 1497220 Tweets processed\n",
      "75000 of 1497220 Tweets processed\n",
      "76000 of 1497220 Tweets processed\n",
      "77000 of 1497220 Tweets processed\n",
      "78000 of 1497220 Tweets processed\n",
      "79000 of 1497220 Tweets processed\n",
      "80000 of 1497220 Tweets processed\n",
      "81000 of 1497220 Tweets processed\n",
      "82000 of 1497220 Tweets processed\n",
      "83000 of 1497220 Tweets processed\n",
      "84000 of 1497220 Tweets processed\n",
      "85000 of 1497220 Tweets processed\n",
      "86000 of 1497220 Tweets processed\n",
      "87000 of 1497220 Tweets processed\n",
      "88000 of 1497220 Tweets processed\n",
      "89000 of 1497220 Tweets processed\n",
      "90000 of 1497220 Tweets processed\n",
      "91000 of 1497220 Tweets processed\n",
      "92000 of 1497220 Tweets processed\n",
      "93000 of 1497220 Tweets processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94000 of 1497220 Tweets processed\n",
      "95000 of 1497220 Tweets processed\n",
      "96000 of 1497220 Tweets processed\n",
      "97000 of 1497220 Tweets processed\n",
      "98000 of 1497220 Tweets processed\n",
      "99000 of 1497220 Tweets processed\n",
      "100000 of 1497220 Tweets processed\n",
      "101000 of 1497220 Tweets processed\n",
      "102000 of 1497220 Tweets processed\n",
      "103000 of 1497220 Tweets processed\n",
      "104000 of 1497220 Tweets processed\n",
      "105000 of 1497220 Tweets processed\n",
      "106000 of 1497220 Tweets processed\n",
      "107000 of 1497220 Tweets processed\n",
      "108000 of 1497220 Tweets processed\n",
      "109000 of 1497220 Tweets processed\n",
      "110000 of 1497220 Tweets processed\n",
      "111000 of 1497220 Tweets processed\n",
      "112000 of 1497220 Tweets processed\n",
      "113000 of 1497220 Tweets processed\n",
      "114000 of 1497220 Tweets processed\n",
      "115000 of 1497220 Tweets processed\n",
      "116000 of 1497220 Tweets processed\n",
      "117000 of 1497220 Tweets processed\n",
      "118000 of 1497220 Tweets processed\n",
      "119000 of 1497220 Tweets processed\n",
      "120000 of 1497220 Tweets processed\n",
      "121000 of 1497220 Tweets processed\n",
      "122000 of 1497220 Tweets processed\n",
      "123000 of 1497220 Tweets processed\n",
      "124000 of 1497220 Tweets processed\n",
      "125000 of 1497220 Tweets processed\n",
      "126000 of 1497220 Tweets processed\n",
      "127000 of 1497220 Tweets processed\n",
      "128000 of 1497220 Tweets processed\n",
      "129000 of 1497220 Tweets processed\n",
      "130000 of 1497220 Tweets processed\n",
      "131000 of 1497220 Tweets processed\n",
      "132000 of 1497220 Tweets processed\n",
      "133000 of 1497220 Tweets processed\n",
      "134000 of 1497220 Tweets processed\n",
      "135000 of 1497220 Tweets processed\n",
      "136000 of 1497220 Tweets processed\n",
      "137000 of 1497220 Tweets processed\n",
      "138000 of 1497220 Tweets processed\n",
      "139000 of 1497220 Tweets processed\n",
      "140000 of 1497220 Tweets processed\n",
      "141000 of 1497220 Tweets processed\n",
      "142000 of 1497220 Tweets processed\n",
      "143000 of 1497220 Tweets processed\n",
      "144000 of 1497220 Tweets processed\n",
      "145000 of 1497220 Tweets processed\n",
      "146000 of 1497220 Tweets processed\n",
      "147000 of 1497220 Tweets processed\n",
      "148000 of 1497220 Tweets processed\n",
      "149000 of 1497220 Tweets processed\n",
      "150000 of 1497220 Tweets processed\n",
      "151000 of 1497220 Tweets processed\n",
      "152000 of 1497220 Tweets processed\n",
      "153000 of 1497220 Tweets processed\n",
      "154000 of 1497220 Tweets processed\n",
      "155000 of 1497220 Tweets processed\n",
      "156000 of 1497220 Tweets processed\n",
      "157000 of 1497220 Tweets processed\n",
      "158000 of 1497220 Tweets processed\n",
      "159000 of 1497220 Tweets processed\n",
      "160000 of 1497220 Tweets processed\n",
      "161000 of 1497220 Tweets processed\n",
      "162000 of 1497220 Tweets processed\n",
      "163000 of 1497220 Tweets processed\n",
      "164000 of 1497220 Tweets processed\n",
      "165000 of 1497220 Tweets processed\n",
      "166000 of 1497220 Tweets processed\n",
      "167000 of 1497220 Tweets processed\n",
      "168000 of 1497220 Tweets processed\n",
      "169000 of 1497220 Tweets processed\n"
     ]
    }
   ],
   "source": [
    "path = \"../../data/archive/2019_03/tweets_\"\n",
    "\n",
    "origin_path = path + \"clean.tsv\"\n",
    "save_path = path + \"processed.tsv\"\n",
    "\n",
    "vocab = preprocess_txt_file(origin_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary length sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate the preprocessed dataset for the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = 'data/hateval2019/hateval2019_es_'\n",
    "save = 'data/hateval2019/hateval2019_clean_es_'\n",
    "files = [\"dev\", \"train\", \"test\"]\n",
    "\n",
    "for file in files:\n",
    "    origin_path = origin + file + \".csv\"\n",
    "    save_path = save + file + \".csv\"\n",
    "\n",
    "    preprocess_DataFrame(origin_path, save_path, \"text\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
